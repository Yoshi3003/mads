{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Discrete Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are hired as a data scientist at International Trade Administration Industry and Analysis National Travel and Tourism Office, a national bureau dedicating to enhancing tourism in the United States, and get involved in the **International Visitation and Spending in the United States** project. Towards the end of a fiscal year, you received a request from the headquarter to obtain insights based on the given tourist visitation number for different states in the U.S. Specifically, you are asked to produce a Jupyter notebook with visualizations that can interact with the 3-year US international visitation data and engage a meeting with various stakeholders, including the headquarter of national travel and tourism in a high-profile video conference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 Load Data (25%)\n",
    "\n",
    "Complete the function `load_data` below to load three datasets that we will use in subsequent questions. Be sure to follow the instructions below for each dataset respectively. \n",
    "\n",
    "* First import the `US_States_Visited_2017.xlsx`, `US_States_Visited_2018.xlsx` and `US_States_Visited_2019.xlsx` datasets. The three datasets are located at the assets folder. You may start with `read_excel()` function in pandas and remove the top and bottom rows.\n",
    "\n",
    "* After that, you will need to multiply all the visitation numbers by 1,000. For example, in 2016, the recorded visitation for Alabama state was supposed to be 141,000 after multiplying 1,000. This must be applied for all 3 datasets.\n",
    "\n",
    "* Finally, you should merge the 3 datasets together, and rename the merged dataset called `merged_US_states_visitation`. The merged dataset should retain only the census states called `state`, 2016 visitation data called `visitation_2016`, 2017 visitation data called `visitation_2017`, 2018 visitation data called `visitation_2018`and 2019 visitation data called `visitation_2019`. To avoid confusion, when we join the datasets, keep every states that ever has international visitation data. Finally, order the state names alphabetically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>visitation_2016</th>\n",
       "      <th>visitation_2017</th>\n",
       "      <th>visitation_2018</th>\n",
       "      <th>visitation_2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>1.240000e+05</td>\n",
       "      <td>136000.0</td>\n",
       "      <td>155545.0</td>\n",
       "      <td>141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135603.0</td>\n",
       "      <td>109000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>1.157751e+06</td>\n",
       "      <td>1035000.0</td>\n",
       "      <td>1168582.0</td>\n",
       "      <td>1196000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California</td>\n",
       "      <td>8.220783e+06</td>\n",
       "      <td>8178000.0</td>\n",
       "      <td>8531051.0</td>\n",
       "      <td>8050000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>4.849022e+05</td>\n",
       "      <td>459000.0</td>\n",
       "      <td>550390.0</td>\n",
       "      <td>509000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>3.232681e+05</td>\n",
       "      <td>303000.0</td>\n",
       "      <td>291149.0</td>\n",
       "      <td>323000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Florida</td>\n",
       "      <td>9.540168e+06</td>\n",
       "      <td>9481000.0</td>\n",
       "      <td>9376578.0</td>\n",
       "      <td>9610000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>8.758310e+05</td>\n",
       "      <td>879000.0</td>\n",
       "      <td>837551.0</td>\n",
       "      <td>868000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Guam</td>\n",
       "      <td>1.582510e+06</td>\n",
       "      <td>1681000.0</td>\n",
       "      <td>1615276.0</td>\n",
       "      <td>1842000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hawaiian Islands</td>\n",
       "      <td>3.146226e+06</td>\n",
       "      <td>3319000.0</td>\n",
       "      <td>3182692.0</td>\n",
       "      <td>3296000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>1.567474e+06</td>\n",
       "      <td>1638000.0</td>\n",
       "      <td>1619264.0</td>\n",
       "      <td>1555000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>2.070000e+05</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>203405.0</td>\n",
       "      <td>226000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>1.130000e+05</td>\n",
       "      <td>144000.0</td>\n",
       "      <td>107685.0</td>\n",
       "      <td>97000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>5.187325e+05</td>\n",
       "      <td>506000.0</td>\n",
       "      <td>498542.0</td>\n",
       "      <td>501000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Maine</td>\n",
       "      <td>1.240000e+05</td>\n",
       "      <td>109000.0</td>\n",
       "      <td>143580.0</td>\n",
       "      <td>149000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>3.758931e+05</td>\n",
       "      <td>428000.0</td>\n",
       "      <td>311090.0</td>\n",
       "      <td>408000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1.642653e+06</td>\n",
       "      <td>1817000.0</td>\n",
       "      <td>1834635.0</td>\n",
       "      <td>1745000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>4.247592e+05</td>\n",
       "      <td>447000.0</td>\n",
       "      <td>494554.0</td>\n",
       "      <td>428000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Minnesota</td>\n",
       "      <td>2.480000e+05</td>\n",
       "      <td>261000.0</td>\n",
       "      <td>283172.0</td>\n",
       "      <td>287000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>2.110000e+05</td>\n",
       "      <td>202000.0</td>\n",
       "      <td>215370.0</td>\n",
       "      <td>170000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>3.416869e+06</td>\n",
       "      <td>3023000.0</td>\n",
       "      <td>3242517.0</td>\n",
       "      <td>3058000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155545.0</td>\n",
       "      <td>105000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>1.105126e+06</td>\n",
       "      <td>1093000.0</td>\n",
       "      <td>1108757.0</td>\n",
       "      <td>1159000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>1.170000e+05</td>\n",
       "      <td>86000.0</td>\n",
       "      <td>131615.0</td>\n",
       "      <td>153000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>New York</td>\n",
       "      <td>1.001379e+07</td>\n",
       "      <td>10287000.0</td>\n",
       "      <td>10804402.0</td>\n",
       "      <td>10518000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>4.360360e+05</td>\n",
       "      <td>525000.0</td>\n",
       "      <td>546402.0</td>\n",
       "      <td>452000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>3.909289e+05</td>\n",
       "      <td>416000.0</td>\n",
       "      <td>422764.0</td>\n",
       "      <td>448000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103697.0</td>\n",
       "      <td>101000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>2.890000e+05</td>\n",
       "      <td>331000.0</td>\n",
       "      <td>307102.0</td>\n",
       "      <td>323000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>9.735632e+05</td>\n",
       "      <td>1004000.0</td>\n",
       "      <td>1013037.0</td>\n",
       "      <td>1054000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139592.0</td>\n",
       "      <td>113000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>2.260000e+05</td>\n",
       "      <td>288000.0</td>\n",
       "      <td>231323.0</td>\n",
       "      <td>246000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>3.909289e+05</td>\n",
       "      <td>409000.0</td>\n",
       "      <td>390857.0</td>\n",
       "      <td>372000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Texas</td>\n",
       "      <td>1.691519e+06</td>\n",
       "      <td>1739000.0</td>\n",
       "      <td>1938331.0</td>\n",
       "      <td>1745000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Utah</td>\n",
       "      <td>6.427773e+05</td>\n",
       "      <td>634000.0</td>\n",
       "      <td>717900.0</td>\n",
       "      <td>739000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>4.210003e+05</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>494554.0</td>\n",
       "      <td>529000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Washington</td>\n",
       "      <td>7.292327e+05</td>\n",
       "      <td>798000.0</td>\n",
       "      <td>797667.0</td>\n",
       "      <td>925000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>1.770000e+05</td>\n",
       "      <td>210000.0</td>\n",
       "      <td>243289.0</td>\n",
       "      <td>234000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2.290000e+05</td>\n",
       "      <td>249000.0</td>\n",
       "      <td>199417.0</td>\n",
       "      <td>246000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               state  visitation_2016  visitation_2017  visitation_2018  \\\n",
       "0            Alabama     1.240000e+05         136000.0         155545.0   \n",
       "1             Alaska              NaN              NaN         135603.0   \n",
       "2            Arizona     1.157751e+06        1035000.0        1168582.0   \n",
       "3         California     8.220783e+06        8178000.0        8531051.0   \n",
       "4           Colorado     4.849022e+05         459000.0         550390.0   \n",
       "5        Connecticut     3.232681e+05         303000.0         291149.0   \n",
       "6            Florida     9.540168e+06        9481000.0        9376578.0   \n",
       "7            Georgia     8.758310e+05         879000.0         837551.0   \n",
       "8               Guam     1.582510e+06        1681000.0        1615276.0   \n",
       "9   Hawaiian Islands     3.146226e+06        3319000.0        3182692.0   \n",
       "10          Illinois     1.567474e+06        1638000.0        1619264.0   \n",
       "11           Indiana     2.070000e+05         195000.0         203405.0   \n",
       "12              Iowa              NaN              NaN              NaN   \n",
       "13          Kentucky     1.130000e+05         144000.0         107685.0   \n",
       "14         Louisiana     5.187325e+05         506000.0         498542.0   \n",
       "15             Maine     1.240000e+05         109000.0         143580.0   \n",
       "16          Maryland     3.758931e+05         428000.0         311090.0   \n",
       "17     Massachusetts     1.642653e+06        1817000.0        1834635.0   \n",
       "18          Michigan     4.247592e+05         447000.0         494554.0   \n",
       "19         Minnesota     2.480000e+05         261000.0         283172.0   \n",
       "20          Missouri     2.110000e+05         202000.0         215370.0   \n",
       "21            Nevada     3.416869e+06        3023000.0        3242517.0   \n",
       "22     New Hampshire              NaN              NaN         155545.0   \n",
       "23        New Jersey     1.105126e+06        1093000.0        1108757.0   \n",
       "24        New Mexico     1.170000e+05          86000.0         131615.0   \n",
       "25          New York     1.001379e+07       10287000.0       10804402.0   \n",
       "26    North Carolina     4.360360e+05         525000.0         546402.0   \n",
       "27              Ohio     3.909289e+05         416000.0         422764.0   \n",
       "28          Oklahoma              NaN              NaN         103697.0   \n",
       "29            Oregon     2.890000e+05         331000.0         307102.0   \n",
       "30      Pennsylvania     9.735632e+05        1004000.0        1013037.0   \n",
       "31      Rhode Island              NaN              NaN         139592.0   \n",
       "32    South Carolina     2.260000e+05         288000.0         231323.0   \n",
       "33         Tennessee     3.909289e+05         409000.0         390857.0   \n",
       "34             Texas     1.691519e+06        1739000.0        1938331.0   \n",
       "35              Utah     6.427773e+05         634000.0         717900.0   \n",
       "36          Virginia     4.210003e+05         440000.0         494554.0   \n",
       "37        Washington     7.292327e+05         798000.0         797667.0   \n",
       "38         Wisconsin     1.770000e+05         210000.0         243289.0   \n",
       "39           Wyoming     2.290000e+05         249000.0         199417.0   \n",
       "\n",
       "    visitation_2019  \n",
       "0          141000.0  \n",
       "1          109000.0  \n",
       "2         1196000.0  \n",
       "3         8050000.0  \n",
       "4          509000.0  \n",
       "5          323000.0  \n",
       "6         9610000.0  \n",
       "7          868000.0  \n",
       "8         1842000.0  \n",
       "9         3296000.0  \n",
       "10        1555000.0  \n",
       "11         226000.0  \n",
       "12         105000.0  \n",
       "13          97000.0  \n",
       "14         501000.0  \n",
       "15         149000.0  \n",
       "16         408000.0  \n",
       "17        1745000.0  \n",
       "18         428000.0  \n",
       "19         287000.0  \n",
       "20         170000.0  \n",
       "21        3058000.0  \n",
       "22         105000.0  \n",
       "23        1159000.0  \n",
       "24         153000.0  \n",
       "25       10518000.0  \n",
       "26         452000.0  \n",
       "27         448000.0  \n",
       "28         101000.0  \n",
       "29         323000.0  \n",
       "30        1054000.0  \n",
       "31         113000.0  \n",
       "32         246000.0  \n",
       "33         372000.0  \n",
       "34        1745000.0  \n",
       "35         739000.0  \n",
       "36         529000.0  \n",
       "37         925000.0  \n",
       "38         234000.0  \n",
       "39         246000.0  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "source_2017_xlsx = \"assets/US_States_Visited_2017.xlsx\"\n",
    "source_2018_xlsx = \"assets/US_States_Visited_2018.xlsx\"\n",
    "source_2019_xlsx = \"assets/US_States_Visited_2019.xlsx\"\n",
    "scale_factor = 1000\n",
    "\n",
    "\n",
    "def scale(value, factor):\n",
    "    return value * factor\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # load data from excel and rename the columns\n",
    "    df_2017 = pd.read_excel(source_2017_xlsx, skiprows=6, usecols=\"B,D,F\", dropna=True).dropna()\n",
    "    df_2017.columns = ['state', 'visitation_2016', 'visitation_2017']\n",
    "    df_2018 = pd.read_excel(source_2018_xlsx, skiprows=7, usecols=\"B,D,G\", dropna=True).dropna()\n",
    "    df_2018.columns = ['state', 'visitation_2018', 'visitation_2017']\n",
    "    df_2019 = pd.read_excel(source_2019_xlsx, skiprows=6, usecols=\"B,D,G\", dropna=True).dropna()\n",
    "    df_2019.columns = ['state', 'visitation_2019', 'visitation_2018']\n",
    "    \n",
    "    # multiply all the visitation numbers by 1,000\n",
    "    df_2017['visitation_2016'] = df_2017['visitation_2016'].apply(scale, factor=scale_factor)\n",
    "    df_2017['visitation_2017'] = df_2017['visitation_2017'].apply(scale, factor=scale_factor)\n",
    "    df_2018['visitation_2017'] = df_2018['visitation_2017'].apply(scale, factor=scale_factor)\n",
    "    df_2018['visitation_2018'] = df_2018['visitation_2018'].apply(scale, factor=scale_factor)\n",
    "    df_2019['visitation_2018'] = df_2019['visitation_2018'].apply(scale, factor=scale_factor)\n",
    "    df_2019['visitation_2019'] = df_2019['visitation_2019'].apply(scale, factor=scale_factor)\n",
    "    \n",
    "    # merge all the datasets together\n",
    "    df_2017['state'] = df_2017['state'].apply(lambda x: x.strip())\n",
    "    df_2018['state'] = df_2018['state'].apply(lambda x: x.strip())\n",
    "    df_2019['state'] = df_2019['state'].apply(lambda x: x.strip())\n",
    "    \n",
    "    df = df_2017.merge(df_2018, how='outer', on='state')\n",
    "    df = df.merge(df_2019, how='outer', on='state')\n",
    "    df.drop(columns=['visitation_2017_y', 'visitation_2018_y'], inplace=True)\n",
    "    df = df[['state', 'visitation_2016', 'visitation_2017_x', 'visitation_2018_x', 'visitation_2019']]\n",
    "    df.columns = ['state', 'visitation_2016', 'visitation_2017', 'visitation_2018', 'visitation_2019']\n",
    "    \n",
    "    # sort the rows by the state name alphabetically\n",
    "    df.sort_values(by=['state'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "merged_US_states_visitation = load_data()\n",
    "merged_US_states_visitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code to validate your creation of the dataframe used for remainder of assignment #1\n",
    "# the two asserts immediately following verify format of dataframe contains the necessary elements\n",
    "assert merged_US_states_visitation.index.size == 40\n",
    "assert all(['visitation_' + str(year) in merged_US_states_visitation.columns for year in [2016, 2017, 2018, 2019]])\n",
    "try:\n",
    "   assert merged_US_states_visitation.iloc[0].name == 'Alabama'\n",
    "except:\n",
    "   assert merged_US_states_visitation['state'].iloc[0] == 'Alabama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL \n",
    "# the below assets match instructor solution, but depending how you cleaned could be little different.\n",
    "# listing the assumptions you make when cleaning data can explain certian differences\n",
    "try:\n",
    "   assert merged_US_states_visitation.loc['Iowa'].isnull().values.any() == True\n",
    "except:\n",
    "   assert merged_US_states_visitation.iloc[12].isnull().values.any() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "   assert merged_US_states_visitation.loc['Michigan'].isnull().values.any() == False\n",
    "except:\n",
    "   assert merged_US_states_visitation.iloc[18].isnull().values.any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert round(merged_US_states_visitation['visitation_2016'].mean(),1) == 1489649.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert round(merged_US_states_visitation['visitation_2017'].mean(),1) == 1507142.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert round(merged_US_states_visitation['visitation_2018'].mean(),1) == 1398576.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert round(merged_US_states_visitation['visitation_2019'].mean(),1) == 1353375.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 Bar Chart (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make use of the merged data to complete the function `make_bar_chart` below. The elements requested by the management team for the first visualization are:\n",
    "* Make 4 plots, each of which is a bar chart representing the total visitation (as y-axis) of each state (shown in x-axis) in year 2016, 2017, 2018 and 2019. Each plot should use the data for each year.\n",
    "* Make the figures readable by adjusting the figure size, and specify the year of each plot using the title (e.g., A proper title of the plot using 2016 visitation data could be something like “Visitation data 2016”.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bar_chart(data):\n",
    "    \n",
    "\n",
    "make_bar_chart(load_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 Transformation (35%)\n",
    "After a week, the management team returned the report back to you can say \"Hey! The visualization looks highly skewed. We could hardly see what is happening in the last few states.\" \n",
    "\n",
    "To better visualize the visitation data to the stakeholders, your manager told you a new requirement: perform **log-transformation** on the visitation number and make the same bar charts again and:\n",
    "\n",
    "* Build the bar chart again with all visitation number log-transformed\n",
    "* (Optional) If you want, you can annotate inside the graphs about the trend you observe in the new subplots. (E.g. In what way does log-transformation improve the visualizations?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transformed_bar_chart(data):   \n",
    "    return None\n",
    "\n",
    "make_transformed_bar_chart(load_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 Zipf's Law on Visitation (Just for fun!)\n",
    "\n",
    "Zipf's law is an empirical law originally proposed by a linguist George Kingsley Zipf to generalize word frequency. Zipf's law states that given a large text corpus with many vocabularies used, the frequency of any word is inversely proportional to its rank in the frequency table. There is a wikipedia page talking about his academic contribution: https://en.wikipedia.org/wiki/George_Kingsley_Zipf\n",
    "\n",
    "For example, **the** is the most frequently occurring word which accounts for nearly 7% of all the words; the runner-up word is **of** which accounts for slightly over 3.5% of words, followed by **and** which accounts for around 2.8%. He observed these patterns and generalized that the $n^{th}$ most frequently occurring word has a frequency of $\\frac{1}{n}$ proportional to the most popular word!\n",
    "\n",
    "Now it's your turn! Do visitation numbers follow the Zipf's law? To answer this, you must make a plot by finishing the function `zipf_approximation_visitation` which \n",
    "* shows the bar chart of international tourist visitation in 2019 for each state sorted descending for the number (you’ve done a bar chart for 2019, now you just need to plot the 2019 visitation number by descending order)\n",
    "* Overlay the Zipf's curve on the graph based on the inverse proportion relationship between visitation and rank (so you need to understand Zipf's law and calculate this)\n",
    "* and finally annotate the image indicating whether or not the tourist visitation approximates the Zipf's law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf_approximation_visitation(data):\n",
    "    return None\n",
    "\n",
    "zipf_approximation_visitation(load_data())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
