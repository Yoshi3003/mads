{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this lecture, we'll be talking about distributions. Specifically, we'll most often be referring to \n",
    "#probability distributions. That is, a function that gives you the likelihood of some outcome occuring,\n",
    "#for every possible state. To make this a bit more concrete, let's look at a canonical example: dice \n",
    "#rolls. Assuming we have a fair die, we'd have a 1/6 chance of landing on any particular side. So, the \n",
    "#probability distribution looks like this:\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(list(range(1,7)), [1/6 for i in range(6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            6-sided die         |         12-sided die              |           120-sided die              |\n",
    "|:------------------------------:| :-------------------------------: | :----------------------------------: |\n",
    "|![6-sided die](assets/6_sided_die.jpg) | ![12-sided die](assets/12_sided_die.jpg) |  ![120-sided die](assets/120_sided_die.jpg) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this case, we only have a fixed number of outcomes. But, what if we have a 12-sided die,\n",
    "#or a 120-sided die? Or a million? The probability of any getting any particular number would get\n",
    "#closer and closer to zero, and we'd essentially be working with continuous data.\n",
    "#In other words, it becomes equally likely for some random variable X to take in outcomes somewhere \n",
    "#between a range of values. Then, we'd end up with something called a uniform distribution, which looks \n",
    "#like a flat top:\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "start = -1\n",
    "stop = 1\n",
    "x_range = stop - start\n",
    "n = 10\n",
    "x = list(np.linspace(start - x_range, start, n+1)) + \\\n",
    "    list(np.linspace(start, stop, n+1)) + \\\n",
    "    list(np.linspace(stop, stop + x_range, n+1))\n",
    "y = [0 for i in range(n+1)] + [1/(x_range) for i in range(n+1)] +  [0 for i in range(n+1)]\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology: PDFs vs. CDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once again, if I were to ask, \"What's the probability at EXACTLY x = 0.5 (with an infinite number of \n",
    "#zeros)?\". Well, the answer would be zero! However, a better question might be, \"What's the probability \n",
    "#of getting a value between 0.4 and 0.6?\" This would simply be the area under the curve between 0.4 and\n",
    "#0.6:\n",
    "\n",
    "import math\n",
    "\n",
    "plt.plot(x, y)\n",
    "print([y[i] for i in range(len(y)) if math.isclose(x[i], 0.4) or math.isclose(x[i], 0.6)])\n",
    "plt.fill_between([0.4, 0.6], 0.5, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Area = h * w = 0.5 * \\frac{1}{0.6-0.4} = 0.1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The plot above is an example of a probability density function, which has the important property\n",
    "#that every y-probability, corresponding to some x, must be between 0 and 1. Also, the total the\n",
    "#total area under the curve MUST sum to one. After all, we're 100% guaranteed to land on some \n",
    "#possible state. For those of you familiar with calculus, in general, the probability of an outcome \n",
    "#between a given range is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(a < X < b) = \\int_a^b pdf(x) dx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#However, don't worry if you aren't sure what an integral is. The main idea you need to know is that the \n",
    "#probability that a random variable takes on a value between some range, is simply the area under that \n",
    "#part of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Something else you might run across is a CDF, or a cumulative distribution function. As the name implies, \n",
    "#this function *accumulates* probability and gives you the total probability of being less than or equal \n",
    "#to some x value. \n",
    "\n",
    "#So for example, what would the corresponding CDF look like for the uniform distribution above?\n",
    "#Well, at -1, we'd have accumulated zero probability, since P(X <= -1) = 0. \n",
    "#Now let's continue along the x-axis. At X=0, we've sort of split the distribution in half,\n",
    "#and since the distribution is uniform, this means there's a 50% chance P(X <= 0).\n",
    "#Finally, the probability P(X <= 1) is 100%, and so, if we plot these points, we get some sort of \n",
    "#linear graph, where the area under the curve looks like a triangle.\n",
    "\n",
    "y_cdf = [0 for i in range(n+1)] + [i/(n+1) for i in range(n+1)] +  [0 for i in range(n+1)]\n",
    "plt.plot(x, y_cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Common Types of Distributions\n",
    "\n",
    "Now, while a uniform distribution is easy to work with, in many practical applications, it's unlikely that every outcome will occur with equal probability. For, instance, we could fabricate some weighted dice. And, while we don't condone cheating on games, this would definitely alter our probability distribution!\n",
    "\n",
    "![weighted dice](assets/weighted.jpg) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, there are numerous ways of weighting dice, some of which are shown in the pictures above. \n",
    "#Let's pretend we end up drilling holes. How might the probability distribution look now?\n",
    "\n",
    "plt.bar(list(range(1,7)), [1/10 if i != 5 else 1/2 for i in range(6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli and Binomial Distributions\n",
    "\n",
    "$$ Bin(k; n,p) = {n \\choose k} p^k (1-p)^{n-k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Of course, these distributions don't necessariliy have to refer to physical objects.\n",
    "#It's not often we come across a weighted coin, but there are many situations that are analogous to\n",
    "#flipping a weighted coin. For instance, say that we run a company that produces battery packs, and\n",
    "#we know that historically, roughly 3 in 1000 is defective. This is essentially flipping a weighted\n",
    "#coin where p = 0.003 (probability of a defect) and 1-p = 0.997 (probability of a working product).\n",
    "\n",
    "#This simple distribution with two outcomes is known as a Bernoulli distribution.\n",
    "#However, say we want to know how likely it is for us to get more than 50 defects out of of our \n",
    "#total batch of 10000 batteries? To answer this, we can turn to the binomial distribution. \n",
    "#The pdf is shown above, where p is the probability of a \"success\" (or in our case, we're defining\n",
    "#this as the probability of a defect), n is the total number of trials, and k is the number of \n",
    "#successes. Fortunately, you won't have to calculate this by hand! We'll use a scipy package to \n",
    "#simplify the process. First, let's plot the pdf for a simpler case:\n",
    "\n",
    "import scipy.stats as stats\n",
    "n, p = 100, 0.4\n",
    "x = np.arange(stats.binom.ppf(0.01, n, p), stats.binom.ppf(0.99, n, p))\n",
    "plt.plot(x, stats.binom.pmf(x, n, p), 'bo', ms=8)\n",
    "plt.xlim(20,60)\n",
    "plt.title(\"Binomial Distribution for N=100, p=0.4\")\n",
    "plt.xlabel(\"Number of successes (k)\")\n",
    "plt.ylabel(\"P(k; n, p)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now, let's say we wanted to get the specific probabilities for our parameters:\n",
    "print(\"P(50; 10000, 0.003) =\", stats.binom.pmf(50, 10000, 0.003))\n",
    "print(\"Probability of getting more than 50 defects:\", 1-stats.binom.cdf(50, 10000, 0.003))\n",
    "\n",
    "#Note that getting more than 50 defects out of a batch of 10,000 isn't very likely at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Distributions\n",
    "\n",
    "You may have noticed that the binomial distribution looks pretty similar to a normal distribution (bell curve). However, there are a few key diferences. For starters, one is a discrete distribution while the other is continuous. But, in a similar procedure discussed in the uniform distribution section above, if we let $N \\rightarrow \\infty$ or become sufficient large, the binomial distribution can be roughly approximated by a normal distribution with the following mean and variance:\n",
    "\n",
    "$$N(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "where\n",
    "$$\\mu = np$$\n",
    "and \n",
    "$$\\sigma^2 = np(1-p)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the pdf for the normal distribution and calculate some simple statistics. \n",
    "#If we use the parameters n=100 and p=0.4 from before...\n",
    "\n",
    "mu = n * p\n",
    "variance = n * p * (1-p)\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100) #create range of x-values +/- 3 stddevs from 0\n",
    "plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
    "\n",
    "#Comparing this to what we had before...it's pretty close! The same functions that applied \n",
    "#before work here as well. \n",
    "\n",
    "prob = stats.norm.cdf(35, mu, variance)\n",
    "print(\"P(x < 35) =\", prob)\n",
    "x_vals = x[x < 35]\n",
    "plt.fill_between(x_vals, 0, stats.norm.pdf(x_vals, mu, sigma), alpha=0.5)\n",
    "\n",
    "#If you want to play around a bit more, here's a link to the documentation:\n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal distributions are frequently encountered in statistics, and we'll leverage our knowledge\n",
    "#of this to conduct statistical tests in the next section. Yet, what happens when we don't have\n",
    "#a good sense of what the population parameters are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population Versus Samples: t-Distribution\n",
    "\n",
    "In the case above, we somehow magically knew the firm's mean number of defects and standard deviation. In that case, looking at normal distribution is appropriate. Another random variable that's normally distributed might be people's heights. If we ask everyone in the world to report their height, we could compute a mean and standard deviation for the entire population. \n",
    "\n",
    "Unfortunately, in most cases, it's probably not feasible to ask every single person we're interested in. Instead, we can only take samples of the whole population. In these cases, we would want to draw inferences not with a normal distribution, but by using something called a t-distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The t-distribution takes in a parameter df, which stands for degrees of freedom. This is equal to \n",
    "#your sample size N, minus 1 (df = N-1). \n",
    "\n",
    "rv1 = stats.t(df=2)\n",
    "x1 = np.linspace(rv1.ppf(0.0001), rv1.ppf(0.9999), 1000)\n",
    "y1 = rv1.pdf(x1)\n",
    "\n",
    "rv2 = stats.t(df=5)\n",
    "x2 = np.linspace(rv2.ppf(0.0001), rv2.ppf(0.9999), 100)\n",
    "y2 = rv2.pdf(x2)\n",
    "\n",
    "rv3 = stats.t(df=20)\n",
    "x3 = np.linspace(rv3.ppf(0.0001), rv3.ppf(0.9999), 100)\n",
    "y3 = rv3.pdf(x3)\n",
    "\n",
    "rv4 = stats.norm()\n",
    "x4 = np.linspace(rv4.ppf(0.0001), rv4.ppf(0.9999), 100)\n",
    "y4 = rv4.pdf(x4)\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.plot(x1,y1, label=\"$df=2$\")\n",
    "plt.plot(x2,y2, label=\"$df=5$\")\n",
    "plt.plot(x3,y3, label=\"$df=20$\")\n",
    "plt.plot(x4,y4, ':', label=\"$N(\\mu=0$, $\\sigma=1)$\")\n",
    "plt.legend()\n",
    "\n",
    "#Notice that the t-distribution has fatter tails than a normal distribution. But, as N approaches \n",
    "#infinity (we have a larger sample size), the t-distribution is closer and closer to being a \n",
    "#standard normal curve (that's a normal distribution with mean 0 and variance 1, which is shown \n",
    "#by the dotted red line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Distributions and the Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from a previous lecture where we talked about standardizing variables or computing a z-score; we can find the z-score of a single sample as follows:\n",
    "\n",
    "$$ z_i = \\frac{x_i - \\mu}{\\sigma} $$\n",
    "\n",
    "However, what if we repeatedly take multiple samples, and then plot the mean of each sample in a so called \"sampling distribution\" instead? What would this distribution even look like? To find out, let's just take a bunch of samples (with the help of a computer of course), and then see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "#try sampling from different distributions\n",
    "iterations = 200\n",
    "n = 30 #sample size\n",
    "x = [np.mean(np.random.randn(n)) for i in range(iterations)]\n",
    "#x = [np.mean(np.random.exponential(scale=1.0, size=n)) for i in range(iterations)]\n",
    "#x = [np.mean(np.random.uniform(low=-2.0, high=2.0, size=n)) for i in range(iterations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the function that will do the plotting, where curr is the current frame\n",
    "def update(curr):\n",
    "    # check if animation is at the last frame, and if so, stop the animation a\n",
    "    if curr == iterations: \n",
    "        a.event_source.stop()\n",
    "    plt.cla()\n",
    "    bins = np.arange(-2, 2, 0.1)\n",
    "    plt.hist(x[:curr], bins=bins)\n",
    "    plt.axis([-2,2,0,60])\n",
    "    plt.gca().set_title('Sampling the Normal Distribution')\n",
    "    plt.gca().set_ylabel('Frequency')\n",
    "    plt.gca().set_xlabel('Value')\n",
    "    plt.annotate('iter = {}\\n n = {}'.format(curr, n), [-1.5,40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "# note: this might take a minute to render!\n",
    "fig = plt.figure()\n",
    "HTML(animation.FuncAnimation(fig, update, interval=50).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the distribution is also normal, and it has mean roughly equal to 0 with a standard deviation of $\\sigma/\\sqrt{n}$, also known as the **standard error**, where n is the size of our samples. The z-score of the sampling distribution is just,\n",
    "\n",
    "$$ z = \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}} .$$\n",
    "\n",
    "This looks almost the same as the previous equation, but with a few key differences. For starters, we're taking a difference between individual sample means ($\\bar{x}$) and the population mean ($\\mu$). Also, we use the standard error instead. \n",
    "\n",
    "The cool thing is that the **Central Limit Theorem** states that this holds true even if we take samples from a non-normal distribution. Feel free to try sampling from the uniform distribution and exponential distributions by uncommenting out the appropriate lines above. One more thing to note, this is only valid if our sample size is sufficiently large. The rule of thumb is than $n \\geq 30$. Try changing this and see what happens!\n",
    "\n",
    "--------------------------\n",
    "\n",
    "When we don't know the population standard deviation (or have really small samples), we have to instead calculate a t-score,\n",
    "\n",
    "$$ t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}} ,$$\n",
    "\n",
    "where $s$ is exactly how you'd calculate a standard deviation normally, just with a slight correction:\n",
    "\n",
    "$$ s = \\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{n-1}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships: Choosing between distributions\n",
    "\n",
    "Now, you might be thinking that this was a lot of material for one lecture! However, keep in mind that you don't need to memorize all of these distributions and their corresponding pdfs and cdfs. The important takeaway is that distributions help us model particular scenarios, and which one you choose will depend on the application you're working on. Also, there's a difference between looking at an entire population of interest, and taking samples from a population. Fortunately, the CLT provides us with some powerful insights that will allow us to compute statistics and draw inferences about your data, which we'll go over in the next section.\n",
    "\n",
    "If you ever find yourself needing some help with picking the correct distribution, here's a list of resources that might help.\n",
    "\n",
    "http://www.math.wm.edu/~leemis/chart/UDR/UDR.html\n",
    "\n",
    "http://people.stern.nyu.edu/adamodar/New_Home_Page/StatFile/statdistns.htm\n",
    "\n",
    "https://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/\n",
    "\n",
    "And just for fun, here's a somewhat complicated diagram showcasing a number of different distributions and how they relate to each other. (There's an even more detailed one in the links!)\n",
    "\n",
    "![distributions](assets/distributions.png) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
