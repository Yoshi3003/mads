{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series and Autocorrelations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this video we'll be talking about time series. And, as the name implies, these are simply a series of\n",
    "data points that are ordered in time. We will first plot a time series and then discuss some features of\n",
    "this graph such as periodicity and lag. After that, we'll use the data to create an autocorrelation plot,\n",
    "which is a helpful tool for checking whether or not we have randomness in the data, or if patterns and\n",
    "correlations actually exist.\n",
    "\n",
    "![time series](assets/timeseries.png)\n",
    "\n",
    "As the image above shows, a time series plot is similar to other graphs you may have already encountered,\n",
    "only that each point along the x-axis represents a point in time. So for instance, the vertical line might\n",
    "represent the actual and 5-day forecasted temperature for August 1, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Throughout this lecture, I'll be using a historical dataset that logs weekly case reports \n",
    "#of the measles disease from 1948-1967 in England and Wales. Let's go ahead and read in \n",
    "#this file and take a look at the data.\n",
    "\n",
    "# Let's bring in pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# And let's read in the dataset\n",
    "df = pd.read_table('assets/ewcitmeas.txt', delim_whitespace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several proprocessing steps we should conduct before moving on to more detailed analyses. For\n",
    "# instance, there are asterisks where we are missing values, and the date, month, and year format isn't\n",
    "# particularly easy to work with. Let's address this by renaming the date-related column names, so that they\n",
    "# are compatible with Panda's to_datetime function, which will automatically generate a single date-time\n",
    "# column, in date-time data types, which are more convenient to work with. The documention can be found here:\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n",
    "df.rename(columns={'DD': 'day', 'MM': 'month', 'YY': 'year'}, inplace=True)\n",
    "# Let's change the year to four digit format\n",
    "df['year'] = df['year'] + 1900\n",
    "# And now let's turn this into an actual datetime field\n",
    "datetime = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df['Datetime'] = datetime\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas has excellent time series data features as it was originally created by Wes McKinney to deal with\n",
    "# financial data. I'll be honest, I don't use time series analyses a lot in my work, so I often find I have to\n",
    "# reference the pandas documentation, but when I do I'm constantly amazed at some of the things you can do\n",
    "# right out of the box.\n",
    "\n",
    "# Let's replace asterisks with 'nan' values, and then use the dropna function to remove these from our\n",
    "# dataframe. Also, if you try to immediately make a plot, you'll notice some weird behavior, which is due to\n",
    "# the fact that the data types of the other columns are actually all strings right now. So, for now, since\n",
    "# we'll only be looking at the cases in London, we'll cast that column to an int.\n",
    "df = df.replace('*', np.nan)\n",
    "df = df.dropna()\n",
    "df['London'] = df['London'].astype('int64')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, data cleaning, it's in every project! :)\n",
    "\n",
    "# Let's bring in pyplot and setup matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# One little trick I'll show you here is that you can set the default figure size using the rcParams\n",
    "# settings. This is a global matplotlib dictionary of default values, and I often find I set a few things\n",
    "# at the top of my notebooks then I can override them if I need to throughout. Here we'll set the figure\n",
    "# size, but you can change default fonts, colors, etc. Note that this must be done after %matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = [16.0,4.0]\n",
    "\n",
    "# Now we're ready to plot the time series, let's look at values for London over time\n",
    "plt.plot(df['Datetime'], df['London'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Periodity and Autocorrelations\n",
    "\n",
    "You may have noticed that there seem to be some spikes that happen every so often in the previous plot, and\n",
    "so one of the questions one might be interested in is whether or not there is some pattern to these\n",
    "increases in reporting. Or in other words, is the time series *periodic* in nature, similar to how the four\n",
    "seasons change in a generally predictable pattern year after year (with some exceptions depending on where\n",
    "you live, of course).\n",
    "\n",
    "Our investigation in this lecture is to comparing an existing time series of data with a time-lagged version\n",
    "of itself (hence the prefix \"auto-\", which means self). If they are correlated, then we'd see a positive\n",
    "value (negative if they are anti-correlated). Otherwise, if there's no relationship, we'd expect to see a\n",
    "value close to zero. \n",
    "\n",
    "We can measure this correlation by \"sliding\" the second function over the first function, and measuring the\n",
    "amount of overlap, like so:\n",
    "\n",
    "![convolution](assets/convolution.gif)\n",
    "\n",
    "If you would like to read more about the details behind how this method works, I highly recommend the\n",
    "wikipedia article on the topic of convolution: https://en.wikipedia.org/wiki/Convolution\n",
    "\n",
    "But, lets do a simple example of our own..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To help solidify the concept of autocorrelation a bit, let's do a bit of autocorrelation by hand. \n",
    "# The first thing we want to do is center our data by subtracting off the mean.\n",
    "x = df['London'] - df['London'].mean()\n",
    "\n",
    "# Now we want to see the correlation between two functions (represented as arrays of values). Since this\n",
    "# is autocorrelation, we're comparing the data in x against itself but we want to shift the comparison\n",
    "# data. We can do this using the mode='full' parameter of numpy's correlate function. The details behind\n",
    "# this are outside the scope of this lecture, but if you would like to learn more about signal comparison\n",
    "# I recommend checking out the scipy.signal docs here:\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate.html\n",
    "autocorr = np.correlate(x, x, mode='full')\n",
    "\n",
    "# The result of this correlation is a new ndarray which is twice the size of x. This new ndarray is\n",
    "# symmetric in that the first half and the second half are mirror images of one another. So for our\n",
    "# analysis we can just look at the second half of this ndarray (which corresponds to the time of lag=0)\n",
    "autocorr = autocorr[x.size:]\n",
    "\n",
    "# Finally, we'll normalize the values between 0 and 1 by dividing by the maximum value, this just makes\n",
    "# things more readable.\n",
    "autocorr /= autocorr.max()\n",
    "\n",
    "# Now we can plot \n",
    "plt.plot(autocorr)\n",
    "# And we'll add a reference line at 0\n",
    "plt.axhline()\n",
    "# And let's toss in some labels\n",
    "plt.xlabel(\"Lag\")\n",
    "plt.ylabel(\"Normalized correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and Interpretation\n",
    "\n",
    "Now that we have an autocorrelation plot, how do we interpret this figure? Well, remember this is a plot of\n",
    "our signal, that is the function (in a mathematics sense) represented by the data from our original figure,\n",
    "where we compare that signal to itself, shifting over time. So there is extremely high correlation at the\n",
    "beginning of the time, since the signals are identical, then as we shift this rapidly drops off as they\n",
    "become dissimilar, and then increases again as both signals spike. By the end of the time frame there is\n",
    "essentially no correlation between the signals.\n",
    "\n",
    "So, what's a reasonable amount of lag? That really depends on your question and the data you are looking at.\n",
    "Let's dig into this a bit more now that we have some idea of how an autocorrelation plot is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I'm going to pivot a bit and just start using the autocorrelation function build into matplotlib. Yup, we\n",
    "# did all of that as a learning exercise, but the library has the functionality already there for us. To use\n",
    "# it we just generate a new axes and figure and then call the acorr() function\n",
    "plt.gca().acorr(x, usevlines=True, maxlags=104, normed=True, lw=2);\n",
    "\n",
    "# Now, acorr() has a couple of parameters I've set here. First, we pass in our data we want to plot. Next, I\n",
    "# set the maxlags value to 104. Since our data is weekly, I want to just zoom in and look at the\n",
    "# autocorrelation for the first two years of data. I also want the data to be normalized, and I set the\n",
    "# usevlines parameter to True. This changes the plot from a scatter graph to a bar plot (or stem plot) which\n",
    "# makes it easier to see the area under the correlation. And lastely, I've put a semicolon at the end of the\n",
    "# line.\n",
    "\n",
    "# What's up with the semicolon? This is actually not python at all, but it's a Jupyter notebook feature. By\n",
    "# default the jupyter notebooks display the last variable in a given cell. That's why when we do something\n",
    "# like df.head() we get a rendered dataframe, because implicitly the jupyter system is changing this to\n",
    "# display(df.head()). This is normally super handy, but in this case, acorr() not only plots our data, but it\n",
    "# returns four different variables: lags, which is a vector of the lag periods, c, which is the\n",
    "# autocorrelation vector, line, which is a number of lines to be plotted, and v, which is the horizontal line\n",
    "# to be plotted. This turns out to be a *lot* of data, so be default Jupyter will send it all to the display.\n",
    "# The semicolon is a handly trick that tells Jupyter to change this default behavior and not display the value\n",
    "# of the last line of the notebook. But our plot should still come out because matplotlib will render it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right, remember, the autocorrelation is mirrored. Let's set out xaxis values to just look at the time\n",
    "# lags we are interested in. Now we don't even need to use that semicolon :)\n",
    "plt.gca().acorr(x, usevlines=True, maxlags=104, normed=True, lw=2)\n",
    "plt.gca().grid(True)\n",
    "plt.gca().set_xlim(0,104)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so, notice that for the first 4 lags, the correlations remain fairly high, above 0.8. This means that\n",
    "measles cases reported for a given week are very similar in number to those in the subsequent 4 weeks (about\n",
    "a month). However, if we look half way through the graph, say around week 52, the number of cases is quite\n",
    "dissimilar. This suggests that there isn't an annual trend in measels cases, that January's number of cases\n",
    "this year will not reflect next January's number of cases. But taking a bigger picture perspective and\n",
    "viewing what's happening at an even larger lag shows that week 104 seems to peak again, which gives evidence\n",
    "that the number of measles cases seems to have a biennial period (or that the data are strongly correlated\n",
    "once every two years). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to come up with another example, because I think autocorrelations are interesting yet can be a bit\n",
    "complex. So I decided to engage in a little experiment and will share that with you here. While I've been\n",
    "preparing some of this lecture I've been capturing my various system statistics such as CPU usage and memory\n",
    "usage to a file. A lot of operating system work is actually based around anticipating or predicting what a\n",
    "user will do, and then trying to intelligently cache data or computation to make it fast. Traditional hard\n",
    "disks are a great example of this, where it is expensive to read a small amount of data, but if you know the\n",
    "user is going to want to read a lot of data you can put it on the disk contiguously to make good use of that\n",
    "space. So, my question was, could we use autocorrelation plots to explore my system usage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at my data, which I captured with the command dstat -tcm --output dstat_siads521.csv\n",
    "df=pd.read_csv(\"assets/dstat_siads521.csv\",skiprows=6)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, our first column is a timestamp, then the next five are CPU statistics, and the last four are memory\n",
    "# statistics. Let's plot some CPU data\n",
    "df=df.set_index(df[\"time\"].apply(lambda x: pd.to_datetime(x[6:])))\n",
    "# Now let's take a minute to look at this data\n",
    "plt.plot(df[\"time\"],df[\"idl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, by eye, I can't see any meaningful time series related trends in that data. There's certainly some\n",
    "# interesting bits, like when I start spotify towards the beginning of my data collection and we see a dip\n",
    "# in CPU availability.\n",
    "\n",
    "# Now let's look over five minutes of data at the idle time of my CPU and show that as an autocorrelation\n",
    "plt.gca().acorr(df['idl'], usevlines=True, maxlags=60*5, normed=True, lw=2)\n",
    "plt.gca().grid(True)\n",
    "plt.gca().set_xlim(0,60*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is pretty interesting, and very different from our previous findings. This decreasing slope suggests\n",
    "that the further you look (the greater you lag) the less the correlation is. So CPU availability is really\n",
    "well predicticted by the most recent CPU availability, but that doesn't describe well what might happen in 5\n",
    "minutes. Whereas with the measles case we found that measels in London was well predictive by what happened\n",
    "in London two years ago, versus one year ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, one more example. Here's some data I captured of my running activity. Specifically, there is a power\n",
    "# level in watts that I captured from my power meter and my heart rate in beats per minute. Let's take a\n",
    "# look\n",
    "df=pd.read_csv(\"assets/stryd.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create out datetime index and do a bit of exploratory data analysis\n",
    "df=df.set_index(df[\"timestamp\"])\n",
    "\n",
    "# Let's plot Power, remember we can set label for a legend and format what we want the line to look like\n",
    "plt.plot(df[\"timestamp\"], df[\"Power\"],'r-', label=\"Power\")\n",
    "# Now we want to put both of these on the same plot, but since they have different scales (power goes over\n",
    "# 200, I sure hope heart_date doesn't!) we want to lock the Axes x-axis but let the Y axis for both float\n",
    "# matplotlib has a handy function for this called twinx()\n",
    "plt.gca().twinx()\n",
    "# Now we can plot heard rate\n",
    "plt.plot(df[\"timestamp\"],df[\"heart_rate\"],'b-', label=\"Heart Rate\")\n",
    "\n",
    "# Now let's put a little labeling on the plot\n",
    "plt.gcf().legend(loc=\"lower center\")\n",
    "plt.gcf().axes[0].set_ylabel(\"Heart Rate\")\n",
    "plt.gcf().axes[1].set_ylabel(\"Power\")\n",
    "plt.title(\"Heart Rate and Power versus Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, so we see that there is some different ramps up for these two data points. While power can be achieved\n",
    "# quickly and seems to flux within a small range, heart rate takes awhile to increase then sort of hits a\n",
    "# maximum. So how would these differ as autocorrelation plots?\n",
    "\n",
    "# Let's create a new figure \n",
    "fig, axs=plt.subplots(2,1,sharex=True,sharey=True,figsize=(12,8))\n",
    "\n",
    "# Now, I'm going to use matplotlib's autocorrelation function, but I'm going to plot the results myself And,\n",
    "# this time around, I'm going to use 15 as my number of observations to look for lag over\n",
    "power_lags,power_auto,*_=plt.acorr(df['Power'],normed=True,maxlags=15)\n",
    "heart_lags,heart_auto,*_=plt.acorr(df['heart_rate'],normed=True,maxlags=15)\n",
    "\n",
    "# Wait, what is this *_ all about!? So, I'm trying to layer in here a few more advanced python features. When\n",
    "# you do tuple unpacking, that is assigning multiple variables to elements in a list like acorr() returns, you\n",
    "# might not want all of the elements. In this case, I only care about the first two variables, and don't care\n",
    "# about the lines and such that acorr() went and created. So I can repack all of these up (using the *) and\n",
    "# assign them to a new variable. In this case, I used the _ as a variable. And that just looks weird, and\n",
    "# that's the point - it's a convention for a junk variable in python. But actually, it's just a variable, so\n",
    "# you could actually go get the lines that were created (feel free to print(_) in the next cell to try it!)\n",
    "\n",
    "# Back to the task at hand. Since we used matplotlib and pyplot, a new figure has been opened. Lets clear it\n",
    "# here so we can do our own thing\n",
    "plt.clf()\n",
    "\n",
    "# Now let's plot our two comparisons as line graphs, since this is mirrored, we start in the middle of each\n",
    "# ndarray\n",
    "i=round(len(power_lags)/2)\n",
    "plt.plot(power_lags[i:],power_auto[i:],'r-',label=\"Power\")\n",
    "plt.plot(heart_lags[i:],heart_auto[i:],'b-',label=\"Heart Rate\")\n",
    "\n",
    "# Now let's put a little labeling on the plot\n",
    "plt.gca().legend(loc=\"lower center\")\n",
    "plt.gca().set_ylabel(\"Scaled Autocorrelation\")\n",
    "plt.gca().set_xlabel(\"Lag\")\n",
    "plt.title(\"Heart Rate and Power Autocorrelations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the autocorrelation for both of these curves follows a similar linearly decreasing trend\n",
    "that we observed in our previous example, but that the power actually has less autocorrelation than the\n",
    "heart rate moment to moment (e.g. over small lag periods). And if you think about this, this seems pretty\n",
    "reasonable physiologically. It's very hard for me to change my heart rate from one moment to the next\n",
    "quickly - my body manages my heart rate for me and scales it up and down as needed. But for power, I can\n",
    "just stop, sending the power to zero, sprint, sending the power high. However, for both of these, there is a\n",
    "norm to a sort of steady state while I'm running. But I wonder, what if I engaged in different running\n",
    "strategies, would that change how the autocorrelation plots look? For instance, when you engage in power\n",
    "training, you aim to keep your power at a certain level, say 300 watts, and to let other variables change\n",
    "depending upon the training conditions. It might be interesting to compare speed, elevation, heart rate, and\n",
    "power together, to explore this relationship further.\n",
    "\n",
    "In this video, I've only scratched the surface when it comes to exploring time series data. For instance,\n",
    "other tools one might use include partial autocorrelation plots and autoregressive integrated moving average\n",
    "(ARIMA) models used for forecasting. This is certainly not the last time you'll see time series data,\n",
    "especially as we become more adept and collecting time series information from our increasingly sensor\n",
    "augmented world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
