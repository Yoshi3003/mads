{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Transforming Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apples to Oranges: Comparing Test Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Let's say we're trying to compare how students perform on college entrance exams at two different high \n",
    "#schools in the US. One is in California, where a large majority of their students tend to take the SAT, and \n",
    "#the other is in Illinois, where students favor taking the ACT. Is there anyway we see how well these school \n",
    "#stack up to one another?\n",
    "\n",
    "#Sunnydale High vs. Shermer High \n",
    "SAT_scores = [690, 330, 600, 350, 540, 440, 650, 480, 570, 420, 360, 620, 580, 600,\n",
    " 390, 420, 510, 640, 350, 470, 570, 430, 410, 420, 380, 420, 510, 620,\n",
    " 470, 700, 520, 560, 480, 540, 450, 550, 520, 460, 410, 550, 400, 350,\n",
    " 780, 590, 510, 410, 520, 340, 430, 370, 560, 560, 500, 560, 490, 550,\n",
    " 430, 520, 710, 520, 460, 390, 550, 410, 480, 450, 520, 610, 380, 620,\n",
    " 530, 460, 460, 660, 520, 580, 490, 560, 520, 380, 440, 610, 530, 350,\n",
    " 630, 440, 450, 590, 430, 640, 500, 290, 560, 390, 320, 470, 700, 540,\n",
    " 440, 550]\n",
    "ACT_scores = [24, 18, 32, 23, 22, 26, 18, 23, 17, 28, 15, 20, 20, 17, 19, 24, 17, 29,\n",
    " 21, 31, 22, 13, 17, 17, 26, 16, 25, 30, 26, 14, 14, 22, 14, 29, 26, 27,\n",
    " 25, 20, 19, 17, 31, 20, 20, 25, 19, 24, 23, 24, 24, 23, 17, 18, 21, 26,\n",
    " 21, 21, 28, 22, 22, 21, 18, 10, 16, 25, 31, 23, 24, 18, 28, 18, 20, 23,\n",
    " 22, 17, 16, 17, 29, 25, 18, 19, 20, 22, 29, 18, 17, 24, 15, 33, 30, 17,\n",
    " 11, 25, 24, 20, 21, 21, 29, 25, 22, 18]\n",
    "\n",
    "columns = [\"SAT\", \"ACT\"]\n",
    "score_df = pd.DataFrame(np.array([SAT_scores, ACT_scores]).T, columns=columns)\n",
    "print(score_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a transformation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do this, we're going to need to transform the data in some way. Specifically, when I talk about a \n",
    "#transformation, all that means is that we're going to apply some function f(x) to each input, and get our \n",
    "#new outputs. Which means, something as simple as x + 0 counts as a transformation, as does the much more \n",
    "#complicated expression below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{f}(k) = \\int_{-\\infty}^{\\infty}f(x) e^{-2\\pi i kx} dx $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Of course, adding zero isn't a particularly *useful* transformation. As for the second one, it's certainly \n",
    "#useful, but not something you'll have to worry about in this course. I'll be sure to point out all the \n",
    "#essential transformations you're going to run across when reading other peoples' analyses, and provide you\n",
    "#with the tools necessary to get started on your own. Now, relating transformations back to our original \n",
    "#question regarding test scores..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Techniques\n",
    "\n",
    "Max-Min Normalization\n",
    "\n",
    "$$ x' = \\frac{x-\\min(x)}{\\max(x)-\\min(x)} $$\n",
    "\n",
    "Standardization (z-score)\n",
    "\n",
    "$$ x' = \\frac{x - \\mu}{\\sigma} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see that the difficulty comes from having two different measures. Scaling allows us to use the same \n",
    "#measuring stick and start to draw some conclusions based on the data. One way of scaling the data is to use \n",
    "#max-min normalization, which transforms all our values between 0 and 1. However, in our particular case, it\n",
    "#might be more helpful to look at standardization, or in other words, to compute z-scores. Recall that this \n",
    "#just captures the difference between your data point and the mean, relative to the spread of the overall \n",
    "#distribution. [read out formula]\n",
    "\n",
    "#In our case, instead of computing mu and sigma for the scores provided, we've looked up the national \n",
    "#averages and standard deviations for both exams so that we can benchmark each school relative to the \n",
    "#rest of the country. \n",
    "\n",
    "#2017 data obtained from https://nces.ed.gov/programs/digest/current_tables.asp (Tables 226.XX)\n",
    "SAT_mean = 527\n",
    "SAT_sd = 107\n",
    "\n",
    "ACT_mean = 20.7\n",
    "ACT_sd = 5.5\n",
    "\n",
    "SAT_norm = (score_df['SAT'] - SAT_mean) / SAT_sd\n",
    "ACT_norm = (score_df['ACT'] - ACT_mean) / ACT_sd\n",
    "normalized_df = pd.DataFrame({'SAT': SAT_norm, 'ACT': ACT_norm})\n",
    "\n",
    "plt.hist(normalized_df['SAT'], bins=12)\n",
    "\n",
    "#Even though we haven't discussed histograms yet, we can still get a vague sense of what's going on here. \n",
    "#Specifically, note that it somewhat resembles a normal distribution: \n",
    "#with a hump--somewhat left of center--that tails off on both ends. \n",
    "\n",
    "#Now let's go ahead and pause for a quick refresher. Try your hand at calculating z-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(normalized_df['ACT'], bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see, the distribution from Shermer High seems to be shifted to the right ever so slightly when \n",
    "#compared to Sunnydale from before. However, just to make sure, let's print out some centrality measures. \n",
    "\n",
    "print(\"Sunnydale Mean:\", normalized_df['SAT'].mean())\n",
    "print(\"Sunnydale Median:\", normalized_df['SAT'].median())\n",
    "\n",
    "print(\"Shermer Mean:\", normalized_df['ACT'].mean())\n",
    "print(\"Shermer Median:\", normalized_df['ACT'].median())\n",
    "\n",
    "#Indeed, both the mean and median for Shermer High are greater than those for Sunnydale, which matches our \n",
    "#intuition. Admittedly, this isn't very rigorous, but it does show how a simple transformation combined \n",
    "#with some basic visual exploration is an effective way of getting some quick insights from your data. At \n",
    "#the end of this module, we'll take a look at some procedures to more confidently answer similar questions\n",
    "#using grounded statistical techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Transformations: Why do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Of course, while we can standardize data, this doesn't necessarily mean that result will actually follow a \n",
    "#bell curve if the data aren't normal to begin with. This can pose a challenge when we want to apply \n",
    "#statistical tools that assume some degree of normality such as z or t-tests. Fortunately, transformations \n",
    "#can help us address this issue as well. \n",
    "#To illustrate, we'll use income distribution data from the 2017 US census. \n",
    "\n",
    "#2017 data obtained from US Census Bureau: Table A-1\n",
    "#(https://www.census.gov/data/tables/2018/demo/income-poverty/p60-263.html)\n",
    "N_households = 127586000\n",
    "income_df = pd.read_csv('assets/income_dist_2017.csv')\n",
    "\n",
    "axis = plt.figure(figsize=(13,5))\n",
    "plt.xticks(rotation=80)\n",
    "plt.bar(income_df['Range'], income_df['Households'])\n",
    "\n",
    "#Now, there are few interesting features we can observe here. First of all, notice that this distribution is \n",
    "#positively, or right, skewed because it *tails* off to the right. With that said, the two bars at the very \n",
    "#end seem a bit out of place. Why is this? Well, each range only spans $5,000, for instance, $35,000 to \n",
    "#$39,999. However, the second to last bin is from $200,000 to $249,999, while $250,000 on up includes \n",
    "#millionaires and billionaires. Though, if we did continue to segement out in $5,000 increments, we might \n",
    "#expect to see a similar decreasing trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For now though, just to keep things simple, let's go ahead and drop those last two bars. Also, just so that \n",
    "#we have integer values on the x-axis to run a transformation, let's just take the midpoint of each bin. \n",
    "\n",
    "income_df = income_df.drop([40, 41])\n",
    "print(income_df['Midpoints'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the data is right skewed, we can use a square-root transformation, or a cube-root transformation. \n",
    "\n",
    "income_df['Transform'] = np.power(income_df['Midpoints'], (1/3))\n",
    "\n",
    "plt.figure(figsize=(13,5))\n",
    "plt.xticks(rotation=80)\n",
    "plt.bar(income_df['Transform'], income_df['Households'])\n",
    "\n",
    "#...and you'll see that the plot looks a lot closer to a normal distribution! Even though this does have the \n",
    "#effect of \"compressing\" some of our points along the right-hand side and creating \"gaps\" towards the left, \n",
    "#this is just because we were given the household counts at specific midpoint values instead of actual data\n",
    "#points, which would resemble something more continuous. We could also try using a histogram, which Nia/I \n",
    "#will delve into in week 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now for a follow-up question: what if we have something that is left skewed?\n",
    "#Here are data obtained from UMich's Acadmeic Reporting Tools (art.ai.umich.edu) for EECS 1281: \n",
    "#Data Structures and Algorithms. Note that we changed the final grades (A+, A, A- all the way down to E) \n",
    "#into their corresponding grade points instead, where we've assigned A+ as 4.3 to see a difference \n",
    "#(even though both A+ and A's are typically counted as a 4.0).\n",
    "\n",
    "N_students = 10312\n",
    "grade_labels = [0, 0.7, 1.0, 1.3, 1.7, 2.0, 2.3, 2.7, 3.0, 3.3, 3.7, 4.0, 4.3]\n",
    "percentages = [0.5, 0.5, 1.1, 2.4, 6.4, 9.2, 7.9, 9.7, 13.4, 15.2, 17.5, 12.3, 4.0]\n",
    "counts = [(x / 100) * N_students for x in percentages]\n",
    "plt.bar(grade_labels, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we took roots for positively-skewed data, it might not be surprising that for negative-skewed data, \n",
    "#we just perform the inverse operation. So, instead of taking a cube root, we'll raise the grade labels to \n",
    "#the third power. \n",
    "\n",
    "transformed_grades = np.power(grade_labels, 3)\n",
    "transformed_grades = (transformed_grades/max(transformed_grades)) * 4.3\n",
    "print(transformed_grades)\n",
    "plt.bar(transformed_grades, counts)\n",
    "\n",
    "#See how we've \"curved\" the grades? Rest assured, there won't be any curving in this course, but even if \n",
    "#there were, we certainly wouldn't be dragging scores downwards!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at another reason for why we might want to transform data. Before we dive into the example, we'll\n",
    "#give you a bit of background knowledge about Newton's second law of motion, which states that force is equal\n",
    "#to mass times acceleration. Rearranging, we see that a = F / m. In other words, the faster you want something \n",
    "#to accelerate, the more force you'll need to apply. And, if you're pushing or pulling with a constant force, \n",
    "#then the more mass an object has, the more it will resist motion (or have a smaller acceleration). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ F = ma \\implies a = \\frac{F}{m}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, what I've mentioned probably aligns with your intuition and isn't very shocking, but let's see what \n",
    "#insights we can gain from this relatively simple equation. To help us out, let's turn to a popular Canadian \n",
    "#sport: curling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![curling](assets/Curling_Canada_Torino_2006.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Say we've given some curlers custom curling stones with various weights ranging from half a kilogram to \n",
    "#20 kilos, have them apply a (roughly) constant force, and then measure the acceleration using some \n",
    "#photosensors. Let's plot the results below:\n",
    "\n",
    "mass = np.arange(0.5, 20, 0.5)\n",
    "acceleration = [99.53, 48.67, 32.21, 26.64, 18.63, 15.92, 15.85, 13.27, 8.57, 6.33, 8.12, 7.12, 5.77, 9.94, 10.3, 3.07, \n",
    "                5.87, 7.69, 5.49, 7.24, 4.45, 5.83, 3.63, 3.35, 2.12, 1.52, 2.82, 3.43, 0.28, 1.38, 2.90, 4.04, 2.97, 2.81, \n",
    "                3.21, 3.51, 2.04, 2.42, 0.89]\n",
    "\n",
    "plt.scatter(mass, acceleration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to find out how much force the curler is applying, but it's kind of difficult to tell just by \n",
    "#looking at the curve above. Can we transform one of the variables to get something that looks a bit easier\n",
    "#to work with? Well, if we take the recipriocal of mass (1/m) and call it x, we'll get a = Fx, which looks \n",
    "#awfully like the equation of a line! Let's try it out:\n",
    "\n",
    "plt.scatter(1 / mass, acceleration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linearizing a function comes with benefits aside from just looking easier to work with. For instance, while \n",
    "#we'll save the details of this for the next lecture, we can plot a trendline and get the slope and intercept\n",
    "#of this line.\n",
    "\n",
    "plt.scatter(1 / mass, acceleration)\n",
    "\n",
    "F, b = np.polyfit(1 / mass, acceleration, 1)\n",
    "abline_values = [F * x + b for x in 1 / mass]\n",
    "plt.plot(1 / mass, abline_values, 'g')\n",
    "\n",
    "print(\"Force:\", F, \"Newtons\") \n",
    "\n",
    "#The slope of the line corresponds to approximately 50 Newtons or 11 pounds of force. Pretty neat. In case \n",
    "#you're interested, we've posted an optional link to a discussion on other fun facts and calculations \n",
    "#regarding curling (http://www.madsci.org/posts/archives/2007-09/1190770482.Ph.r.html). So, to recap, \n",
    "#transformations allow us to linarize functions, which may be easier to manipulate, or allow us to \n",
    "#extrapolate other details of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations: Making life easier (or harder)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, at this point, you might be thinking that transformations feel a bit \"unnatural\" or that it's not quite \n",
    "#clear what technique to use, when. But, transformations do often work well in practice, and you'll gain\n",
    "#intuition as we work through more examples throughout this course. In fact, I'll bet you were probably \n",
    "#familiar with this concept even before this lecture. For instance, many real-life scales involve \n",
    "#transformations, such as: the pH scale (used for measuring acidity levels), the Richter scale used for \n",
    "#measuring the magnitude of an earthquake (although the modern standard is actually the moment-magnitude scale), \n",
    "#and the decibel scale used for measuring sound levels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decibel Scale:\n",
    "\n",
    "| Source of Sound                          | Sound Pressure ($\\mu$Pa)  | Decibels  |\n",
    "| ---------------------------------------- |:-------------------------:| ---------:|\n",
    "| Launching of a space shuttle\t           |     2,000,000,000         |   180     |\n",
    "| Full symphony orchestra\t               |       2,000,000           |   100     |\n",
    "| High speed diesel freight train at 25 m  |         200,000           |    80     |\n",
    "| Normal conversation\t                   |          20,000           |    60     |\n",
    "| Soft whispering at 2m in a library       |           2,000           |    40     |\n",
    "| Unoccupied broadcast studio              |            200            |    20     |\n",
    "| Threshold of hearing                     |             20\t           |     0     |\n",
    "\n",
    "Adapted from: https://www.epd.gov.hk/epd/noise_education/web/ENG_EPD_HTML/m1/intro_5.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that the extreme range of values, like sound pressure in this case (measured in micropascals), is \n",
    "#rather inconvinient to talk about, because it spans several orders of magnitude. That's why we discuss how \n",
    "#loud sound is in terms of decibels. With that said, it's important to note that going from 40 decibels to \n",
    "#80 decibels does NOT mean that the source becomes twice as loud. Since the scale is log transformed, the \n",
    "#difference is actually 100-fold! That's equivalent to going from a soft whisper to having a diesel freight \n",
    "#train chugging away nearby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study: New York Stock Exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll end this lecture by going through a case study. The following data are the closing values for the Dow\n",
    "#Jones Industrial Average (DJIA), a stock market index of 30 large, publicly owned companies based in the US\n",
    "#from 1915 to 2018. Note that the most recent global 2008 recession is clearly depicted. Can you identify any \n",
    "#other recessions? How we would identify periods of economic downturn or stagnation in general?\n",
    "\n",
    "#Data obtained from https://www.macrotrends.net/1319/dow-jones-100-year-historical-chart \n",
    "years = np.arange(1915, 2019, 1) \n",
    "closing_values = [99.15, 95.00, 74.38, 82.20, 107.23, 71.95, 80.80, 98.17, 95.52, 120.51, 151.08, 157.20, 200.70, 300.00, \n",
    "                  248.48, 164.58, 77.90, 59.93, 99.90, 104.04, 144.13, 179.90, 120.85, 154.76, 150.24, 131.13, 110.96, 119.40, \n",
    "                  135.89, 152.32, 192.91, 177.20, 181.16, 177.30, 200.13, 235.41, 269.23, 291.90, 280.90, 404.39, 488.40, \n",
    "                  499.47, 435.69, 583.65, 679.36, 615.89, 731.14, 652.10, 762.95, 874.13, 969.26, 785.69, 905.11, 943.75, \n",
    "                  800.36, 838.92, 890.20, 1020.02, 850.86, 616.24, 852.41, 1004.65, 831.17, 805.01, 838.74, 963.99, 875.00, \n",
    "                  1046.54, 1258.64, 1211.57, 1546.67, 1895.95, 1938.83, 2168.57, 2753.20, 2633.66, 3168.83, 3301.11, 3754.09, \n",
    "                  3834.44, 5117.12, 6448.27, 7908.30, 9181.43, 11497.12, 10787.99, 10021.57, 8341.63, 10453.92, 10783.01, \n",
    "                  10717.50, 12463.15, 13264.82, 8776.39, 10428.05, 11577.51, 12217.56, 13104.14, 16576.66, 17823.07, 17425.03, \n",
    "                  19762.60, 24719.22, 23327.46]\n",
    "\n",
    "plt.plot(years, closing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          Crowds outside NYSE after crash       | \"Bank runs\"       |\n",
    "|:-----------------:| :------------------------: |\n",
    "|  ![Crowds outside NYSE](assets/Crowd_NYSE.jpg)        | ![American Union Bank](assets/American_Union_Bank.png)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For those of you who are familiar with US history, you may know that there was a severe worldwide economic \n",
    "#downturn in the 1930s, after WWI and before the start of WWII. The unemployment rate reached 25%, banks \n",
    "#began to fail, and a swaths of people lined up to withdraw whatever savings they had left, as depicted\n",
    "#above. The \"Great Depression\", as it's called, doesn't seem to appear in our plot though. Why is that?\n",
    "\n",
    "#For starters, the Dow Jones used to be measured in the hundereds, whereas in modern times, we're talking \n",
    "#about tens of thousands of points. Just the daily fluctuations might exceed a hundred points, thereby \n",
    "#obscuring all the details in the left-hand region of the plot. Since the rates of change seem to be \n",
    "#proportional to the index's current value, it might be worth exploring a logarithmic transformation. \n",
    "#Let's see what happens when we do that:\n",
    "\n",
    "log_closing_values = np.log(closing_values)\n",
    "plt.plot(years, log_closing_values)\n",
    "\n",
    "#Voila! Now the Great Depression is clearly visible. Even though a change of 30 points may seem minisciule \n",
    "#nowadays, on October 29, 1929, or \"Black Tuesday\", this was a 12% decrease, which accounts for a significant \n",
    "#portion of that giant dropoff in the left-hand side of the graph. Note that there are a few other periods \n",
    "#where the market seems to stagnate, and while it's still difficult to precisely pinpoint every major \n",
    "#recession, we are able to make out a lot more intricacies of the data, whereas this was all shrouded before\n",
    "#we applied the transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which transformation should I choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not a \"one size fits all\" process, should start by exploring your data \n",
    "- Normalizing data is a common and sometimes necessary transformation for applying later steps in a statistical pipeline\n",
    "- Can sometimes reduce skewness by applying square/square root transformations (Nia will come back to this in the context of histograms in week 2!)\n",
    "- Reciprocal and logarithmic transformation are other useful transformations to know\n",
    "- These transformations have visual effects: the right choice might make analysis easier or emphasize different features of the data\n",
    "- Very useful resource for much of the information in this lecture http://fmwww.bc.edu/repec/bocode/t/transint.html\n",
    "- In the next section, we'll look at some ways to spruce up our graphs (previewed here), such as drawing trendlines, highlighting regions, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
